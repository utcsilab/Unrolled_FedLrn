{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sigpy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright (c) Facebook, Inc. and its affiliates.\n",
    "This source code is licensed under the MIT license found in the\n",
    "LICENSE file in the root directory of this source tree.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Block that consists of two convolution layers each followed by\n",
    "    instance normalization, LeakyReLU activation and dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, drop_prob):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input.\n",
    "            out_chans (int): Number of channels in the output.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.Dropout2d(drop_prob),\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.Dropout2d(drop_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        return self.layers(input)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'ConvBlock(in_chans={self.in_chans}, out_chans={self.out_chans}, ' \\\n",
    "            f'drop_prob={self.drop_prob})'\n",
    "\n",
    "\n",
    "class TransposeConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transpose Convolutional Block that consists of one convolution transpose layers followed by\n",
    "    instance normalization and LeakyReLU activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input.\n",
    "            out_chans (int): Number of channels in the output.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_chans, out_chans, kernel_size=2, stride=2, bias=False),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        return self.layers(input)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'ConvBlock(in_chans={self.in_chans}, out_chans={self.out_chans})'\n",
    "\n",
    "\n",
    "# class up_conv(nn.Module):\n",
    "#     def __init__(self, chans, num_pool_layers, drop_prob, sites):\n",
    "    \n",
    "\n",
    "class Branched_UnetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a U-Net model.\n",
    "    This is based on:\n",
    "        Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks\n",
    "        for biomedical image segmentation. In International Conference on Medical image\n",
    "        computing and computer-assisted intervention, pages 234–241. Springer, 2015.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, chans, num_pool_layers, drop_prob, sites):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input to the U-Net model.\n",
    "            out_chans (int): Number of channels in the output to the U-Net model.\n",
    "            chans (int): Number of output channels of the first convolution layer.\n",
    "            num_pool_layers (int): Number of down-sampling and up-sampling layers.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.sites = sites\n",
    "        \n",
    "        self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, chans, drop_prob)])\n",
    "        ch = chans\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.down_sample_layers += [ConvBlock(ch, ch * 2, drop_prob)]\n",
    "            ch *= 2\n",
    "        self.conv = ConvBlock(ch, ch * 2, drop_prob)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.up_conv = nn.ModuleList()\n",
    "        self.up_transpose_conv = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.up_transpose_conv += [TransposeConvBlock(ch * 2, ch)]\n",
    "            self.up_conv += [ConvBlock(ch * 2, ch, drop_prob)]\n",
    "            ch //= 2\n",
    "\n",
    "        self.up_transpose_conv += [TransposeConvBlock(ch * 2, ch)]\n",
    "        self.up_conv += [\n",
    "            nn.Sequential(\n",
    "                ConvBlock(ch * 2, ch, drop_prob),\n",
    "                nn.Conv2d(ch, self.out_chans, kernel_size=1, stride=1),\n",
    "            )]\n",
    " \n",
    "       #ModuleLists for storing site specific Decoder weights\n",
    "        self.Decoder_up_list = torch.nn.ModuleList(\n",
    "                        [ self.up_conv  for i in range(self.sites)])\n",
    "    \n",
    "        self.Decoder_up_trans_list = torch.nn.ModuleList(\n",
    "                        [ self.up_transpose_conv  for i in range(self.sites)])\n",
    "\n",
    "        \n",
    "    def forward(self, site, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        stack = []\n",
    "        output = input.unsqueeze(1)\n",
    "\n",
    "        # Apply down-sampling layers\n",
    "        for i, layer in enumerate(self.down_sample_layers):\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "            output = F.avg_pool2d(output, kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        output = self.conv(output)\n",
    "\n",
    "        # Apply up-sampling layers\n",
    "        for transpose_conv, conv in zip(self.Decoder_up_trans_list[site], self.Decoder_up_list[site]):\n",
    "            downsample_layer = stack.pop()\n",
    "            output = transpose_conv(output)\n",
    "\n",
    "            # Reflect pad on the right/botton if needed to handle odd input dimensions.\n",
    "            padding = [0, 0, 0, 0]\n",
    "            if output.shape[-1] != downsample_layer.shape[-1]:\n",
    "                padding[1] = 1 # Padding right\n",
    "            if output.shape[-2] != downsample_layer.shape[-2]:\n",
    "                padding[3] = 1 # Padding bottom\n",
    "            if sum(padding) != 0:\n",
    "                output = F.pad(output, padding, \"reflect\")\n",
    "\n",
    "            output = torch.cat([output, downsample_layer], dim=1)\n",
    "            output = conv(output)\n",
    "            \n",
    "        return output.squeeze(1)\n",
    "\n",
    "    \n",
    "class OldUnetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a U-Net model.\n",
    "    This is based on:\n",
    "        Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks\n",
    "        for biomedical image segmentation. In International Conference on Medical image\n",
    "        computing and computer-assisted intervention, pages 234–241. Springer, 2015.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, chans, num_pool_layers, drop_prob):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input to the U-Net model.\n",
    "            out_chans (int): Number of channels in the output to the U-Net model.\n",
    "            chans (int): Number of output channels of the first convolution layer.\n",
    "            num_pool_layers (int): Number of down-sampling and up-sampling layers.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, chans, drop_prob)])\n",
    "        ch = chans\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.down_sample_layers += [ConvBlock(ch, ch * 2, drop_prob)]\n",
    "            ch *= 2\n",
    "        self.conv = ConvBlock(ch, ch * 2, drop_prob)\n",
    "\n",
    "        self.up_conv = nn.ModuleList()\n",
    "        self.up_transpose_conv = nn.ModuleList()\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.up_transpose_conv += [TransposeConvBlock(ch * 2, ch)]\n",
    "            self.up_conv += [ConvBlock(ch * 2, ch, drop_prob)]\n",
    "            ch //= 2\n",
    "\n",
    "        self.up_transpose_conv += [TransposeConvBlock(ch * 2, ch)]\n",
    "        self.up_conv += [\n",
    "            nn.Sequential(\n",
    "                ConvBlock(ch * 2, ch, drop_prob),\n",
    "                nn.Conv2d(ch, self.out_chans, kernel_size=1, stride=1),\n",
    "            )]\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        stack = []\n",
    "        output = input.unsqueeze(1)\n",
    "\n",
    "        # Apply down-sampling layers\n",
    "        for i, layer in enumerate(self.down_sample_layers):\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "            output = F.avg_pool2d(output, kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        output = self.conv(output)\n",
    "\n",
    "        # Apply up-sampling layers\n",
    "        for transpose_conv, conv in zip(self.up_transpose_conv, self.up_conv):\n",
    "            downsample_layer = stack.pop()\n",
    "            output = transpose_conv(output)\n",
    "\n",
    "            # Reflect pad on the right/botton if needed to handle odd input dimensions.\n",
    "            padding = [0, 0, 0, 0]\n",
    "            if output.shape[-1] != downsample_layer.shape[-1]:\n",
    "                padding[1] = 1 # Padding right\n",
    "            if output.shape[-2] != downsample_layer.shape[-2]:\n",
    "                padding[3] = 1 # Padding bottom\n",
    "            if sum(padding) != 0:\n",
    "                output = F.pad(output, padding, \"reflect\")\n",
    "\n",
    "            output = torch.cat([output, downsample_layer], dim=1)\n",
    "            output = conv(output)\n",
    "\n",
    "        return output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW MODEL!!!!!!!!!!!!\n",
      "down_sample_layers.0.layers.0.weight\n",
      "down_sample_layers.0.layers.4.weight\n",
      "down_sample_layers.1.layers.0.weight\n",
      "down_sample_layers.1.layers.4.weight\n",
      "down_sample_layers.2.layers.0.weight\n",
      "down_sample_layers.2.layers.4.weight\n",
      "down_sample_layers.3.layers.0.weight\n",
      "down_sample_layers.3.layers.4.weight\n",
      "conv.layers.0.weight\n",
      "conv.layers.4.weight\n",
      "up_conv.0.layers.0.weight\n",
      "up_conv.0.layers.4.weight\n",
      "up_conv.1.layers.0.weight\n",
      "up_conv.1.layers.4.weight\n",
      "up_conv.2.layers.0.weight\n",
      "up_conv.2.layers.4.weight\n",
      "up_conv.3.0.layers.0.weight\n",
      "up_conv.3.0.layers.4.weight\n",
      "up_conv.3.1.weight\n",
      "up_conv.3.1.bias\n",
      "up_transpose_conv.0.layers.0.weight\n",
      "up_transpose_conv.1.layers.0.weight\n",
      "up_transpose_conv.2.layers.0.weight\n",
      "up_transpose_conv.3.layers.0.weight\n",
      "Decoder_up_list.0.0.layers.0.weight\n",
      "Decoder_up_list.0.0.layers.4.weight\n",
      "Decoder_up_list.0.1.layers.0.weight\n",
      "Decoder_up_list.0.1.layers.4.weight\n",
      "Decoder_up_list.0.2.layers.0.weight\n",
      "Decoder_up_list.0.2.layers.4.weight\n",
      "Decoder_up_list.0.3.0.layers.0.weight\n",
      "Decoder_up_list.0.3.0.layers.4.weight\n",
      "Decoder_up_list.0.3.1.weight\n",
      "Decoder_up_list.0.3.1.bias\n",
      "Decoder_up_list.1.0.layers.0.weight\n",
      "Decoder_up_list.1.0.layers.4.weight\n",
      "Decoder_up_list.1.1.layers.0.weight\n",
      "Decoder_up_list.1.1.layers.4.weight\n",
      "Decoder_up_list.1.2.layers.0.weight\n",
      "Decoder_up_list.1.2.layers.4.weight\n",
      "Decoder_up_list.1.3.0.layers.0.weight\n",
      "Decoder_up_list.1.3.0.layers.4.weight\n",
      "Decoder_up_list.1.3.1.weight\n",
      "Decoder_up_list.1.3.1.bias\n",
      "Decoder_up_list.2.0.layers.0.weight\n",
      "Decoder_up_list.2.0.layers.4.weight\n",
      "Decoder_up_list.2.1.layers.0.weight\n",
      "Decoder_up_list.2.1.layers.4.weight\n",
      "Decoder_up_list.2.2.layers.0.weight\n",
      "Decoder_up_list.2.2.layers.4.weight\n",
      "Decoder_up_list.2.3.0.layers.0.weight\n",
      "Decoder_up_list.2.3.0.layers.4.weight\n",
      "Decoder_up_list.2.3.1.weight\n",
      "Decoder_up_list.2.3.1.bias\n",
      "Decoder_up_trans_list.0.0.layers.0.weight\n",
      "Decoder_up_trans_list.0.1.layers.0.weight\n",
      "Decoder_up_trans_list.0.2.layers.0.weight\n",
      "Decoder_up_trans_list.0.3.layers.0.weight\n",
      "Decoder_up_trans_list.1.0.layers.0.weight\n",
      "Decoder_up_trans_list.1.1.layers.0.weight\n",
      "Decoder_up_trans_list.1.2.layers.0.weight\n",
      "Decoder_up_trans_list.1.3.layers.0.weight\n",
      "Decoder_up_trans_list.2.0.layers.0.weight\n",
      "Decoder_up_trans_list.2.1.layers.0.weight\n",
      "Decoder_up_trans_list.2.2.layers.0.weight\n",
      "Decoder_up_trans_list.2.3.layers.0.weight\n",
      "OLD MODEL!!!!!!!!!!!!\n",
      "down_sample_layers.0.layers.0.weight\n",
      "down_sample_layers.0.layers.4.weight\n",
      "down_sample_layers.1.layers.0.weight\n",
      "down_sample_layers.1.layers.4.weight\n",
      "down_sample_layers.2.layers.0.weight\n",
      "down_sample_layers.2.layers.4.weight\n",
      "down_sample_layers.3.layers.0.weight\n",
      "down_sample_layers.3.layers.4.weight\n",
      "conv.layers.0.weight\n",
      "conv.layers.4.weight\n",
      "up_conv.0.layers.0.weight\n",
      "up_conv.0.layers.4.weight\n",
      "up_conv.1.layers.0.weight\n",
      "up_conv.1.layers.4.weight\n",
      "up_conv.2.layers.0.weight\n",
      "up_conv.2.layers.4.weight\n",
      "up_conv.3.0.layers.0.weight\n",
      "up_conv.3.0.layers.4.weight\n",
      "up_conv.3.1.weight\n",
      "up_conv.3.1.bias\n",
      "up_transpose_conv.0.layers.0.weight\n",
      "up_transpose_conv.1.layers.0.weight\n",
      "up_transpose_conv.2.layers.0.weight\n",
      "up_transpose_conv.3.layers.0.weight\n"
     ]
    }
   ],
   "source": [
    "print('NEW MODEL!!!!!!!!!!!!')\n",
    "new_model = Branched_UnetModel(in_chans = 2 , out_chans = 2, chans = 32, num_pool_layers = 4, drop_prob = 0.0, sites = 3)\n",
    "for key in new_model.state_dict():\n",
    "    print(key)\n",
    "    \n",
    "print('OLD MODEL!!!!!!!!!!!!')\n",
    "old_model = OldUnetModel(in_chans = 2 , out_chans = 2, chans = 32, num_pool_layers = 4, drop_prob = 0.0)\n",
    "for key in old_model.state_dict():\n",
    "    print(key)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 7756418\n",
      "Total parameters 7756418\n"
     ]
    }
   ],
   "source": [
    "# Count parameters\n",
    "total_params_new = np.sum([np.prod(p.shape) for p\n",
    "                           in new_model.parameters() if p.requires_grad])\n",
    "print('Total parameters %d' % total_params_new)\n",
    "\n",
    "total_params_old = np.sum([np.prod(p.shape) for p\n",
    "                           in old_model.parameters() if p.requires_grad])\n",
    "print('Total parameters %d' % total_params_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 512, 3, 3])\n",
      "torch.Size([256, 512, 3, 3])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256, 256, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(new_model.state_dict()['Decoder_up_list.0.0.layers.0.weight'].shape)\n",
    "print(new_model.state_dict()['up_conv.0.layers.0.weight'].shape)\n",
    "\n",
    "print(new_model.state_dict()['Decoder_up_list.0.0.layers.4.weight'].shape)\n",
    "print(new_model.state_dict()['up_conv.0.layers.4.weight'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Nested ModuleLists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 420\n"
     ]
    }
   ],
   "source": [
    "outs = nn.ModuleList([nn.ModuleList([nn.Conv2d(3, 3, 3, 1, 1) for j in range(5)]) for i in range(1)])\n",
    "total_params = np.sum([np.prod(p.shape) for p\n",
    "                           in outs.parameters() if p.requires_grad])\n",
    "print('Total parameters %d' % total_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
